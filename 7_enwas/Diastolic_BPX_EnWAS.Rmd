---
title: "EnWAS Work Flow"
date: "Updated on : `r date()`"
output:
  bookdown::html_document2: default

---
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Objective

This notebook demonstrates an Environment‚ÄêWide Association Study (EnWAS) work flow shown in Figure \@ref(fig:workflow). 

```{r workflow, fig.cap = "EnWAS Work Flow",fig.align = 'center',out.width = '90%',echo=FALSE}
knitr::include_graphics("process.png")
```
In this example, we are interested in finding nutrition predictors associated with diastolic blood pressure. To show work with the above steps, we will perform a linear regression  model and a linear regression with applying natural spline functions on the continuous variables. In short, a spline-based linear regression model can capture the non-linearity trend of the data by using piece-wise polynomial functions on continuous variables. More information about spline can be found on [wikipedia](https://en.wikipedia.org/wiki/Spline_(mathematics))


# Identify a response and a set of confounders that will be adjusted for

## Identify The Data Source

We will use the [NHANES](https://www.cdc.gov/nchs/nhanes/index.htm) as our data source more details about data can be found on the CDC website. A quick start notebook about accessing the data in our docker database with provided R function shown in [here](https://ccb-hms.github.io/Imputation/6_phonto/quick_start.html).


## Identify a response, or outcome of interest.
In this demonstration, we choose diastolic [blood pressure](https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/BPX_I.htm#BPXDI1) as the outcome interest. These data are
contained in tables in the NHANES database.

For each survey a number of people were chosen to provide blood pressure measurements and these were obtained in duplicate on two separate days.
To get an more accurate measurement of the diastolic blood pressure, we average two times of measurements for each participants.

## Identify a set of confounders that will be adjusted for.  These are typically not of interest themselves, but are important established confounders such as age or sex that should be adjusted for.

Based on our literature review we feel that sex, age, ethnicity, education and BMI are likely to affect blood pressure, and we choose them as the confounders to adjust for.  In a more realistic analysis we would likely also want to find out who is taking medications that are intended to alter blood pressure, but for simplicity of the exposition we will ignore
that issue in this analysis.

Normally the model fitting and QA/QC process would be carried out in an exploratory and iterative fashion, however that is not easy to capture in a static document.  Instead we will compare two basic models, one where we assume that there is a straight line relationship between continuous confounders and our response, and a second approach where we use spline 
models.  This approach has some similarities to the Generalized Additive Model (GAMs) and much of the literature there is useful. There are some differences though, and we will outline them as we go.


```{r setup,warning=FALSE,message=FALSE, echo=F}
library(EnWAS)
library(splines)
library(ggplot2)
library(ggpubr)
library(dplyr)
library(DBI)
library(phonto)
library(DT)
library(kableExtra)
# setup database configuration
sqlHost <- "localhost"
sqlUserName <- "sa"
sqlPassword <- "yourStrong(!)Password"
sqlDefaultDb <- "NhanesLandingZone"
```


## Load data that contains responses and confounders, and preprocess using PHONTO 

We load the NHANES data using functions from [PHONTO package](https://github.com/ccb-hms/phonto). The PHONTO package provides a set function to facilitate researchers to get access NHANES data in our docker database.  More detail about data query and search tools can be found on [quick start page](https://ccb-hms.github.io/Imputation/6_phonto/quick_start.html).
In addition, the `phesant()` function allows the user to check the data inspired by the [PHESANT package](https://github.com/MRCIEU/PHESANT).

```{r phonto}
cols = c("RIDAGEYR","RIAGENDR","BMXBMI","RIDRETH1","DMDEDUC2","years",'BPXDI1','BPXDI2')
base_df = jointQuery(c('BodyMeasures','DemographicVariablesAndSampleWeights','BloodPressure'),cols)
phs_res = phesant(base_df)
phs_res$phs_res |> kbl(caption = "PHESANT-like Process Results",align='c') |>
  kable_classic(full_width = T, html_font = "Cambria")
```
Table \@ref(tab:phonto) shows variables names, the ratio of unique values (`r_unique`), the proportion of zeros (`r_zeros`), and the ratio of NAs (`r_NAs`), which is calculated by the number of unique values, zeros, and NAs divided by total records. The categorical data types (ordered or unrecorded) are presented by integers, and the PHESANT function category them as multilevel. The data are categorized as continuous (doubles, integers) and multilevel(characters, integers with unique values less than 20), as shown type column. For example, education (DMDEDUC2) is category as Multilevel(8) means the PHESANT process considers it multilevel and has 8 levels; moreover, `r_NAs` shows that almost 50% of education data is missing, and we recommend users check data in the cases like this. `r_zeros` shows that about 0.8% blood pressure measurements are filled as 0s. We may want to remove those data records because blood pressure with 0s is for dead people.

We remove missing values (NAs) and blood pressure filled with 0s.
In addition, the gender(`RIAGENDR`) and ethnicity (`RIDRETH1`) are converted to categorical variables.

```{r}
# remove age under 20 and diastolic with 0s
base_df = base_df |> subset(RIDAGEYR>20 & BPXDI1!=0 & BPXDI2!=0 )
# assign the gender and ethnicity to the real levels
base_df = nhanesA::nhanesTranslate('DEMO_I', c('RIAGENDR', 'RIDRETH1'), data=base_df)
# Transform education to:
#  < High School
#   - High School
#  > High School
nDEDUC = ifelse(base_df$DMDEDUC2 < 3, "<HS", ifelse(base_df$DMDEDUC2 == 3, "HS", 
                                                    ifelse(base_df$DMDEDUC2 < 6, ">HS", NA)))
base_df$DMDEDUC2 <- as.factor(nDEDUC)
base_df = na.omit(base_df)
# Take average first and second read for the diastolic blood pressure.
base_df$DIASTOLIC <- (base_df$BPXDI1+base_df$BPXDI2)/2
```

# Build Baseline Model

In the following demonstrations, we built a linear model and a spline model to show that spline models outperform the linear when we have continuous variables. The outcome is diastolic is the average of the diastolic first (`BPXDI1`) and second (`BPXDI2`) reads, gender(`RIAGENDR`), age (`RIDAGEYR`), ethnicity (`RIDRETH1`), BMI(`BMXBMI`) and the ratio of family income to poverty (`INDFMPIR`).


```{r build_base_model,echo=TRUE,results = "asis"}
lm_str <- 'DIASTOLIC ~ RIDAGEYR*RIAGENDR + BMXBMI + RIDRETH1+DMDEDUC2+years'
lm_base <- lm(formula = as.formula(lm_str),base_df)
ns_str <-
  'DIASTOLIC ~ ns(RIDAGEYR, knots = seq(30, 80, by = 10), Boundary.knots=c(20,90)) * RIAGENDR + ns(BMXBMI,knots = c(seq(15, 45, by = 5),seq(45,65,by=10)),Boundary.knots=c(10,85)) + RIDRETH1+DMDEDUC2+years'
ns_base <- lm(formula = as.formula(ns_str), base_df)
```



# QA/QC for Base Model

We need to check the base model and ensure it runs correctly before performing EnWAS. However, the classical methods such as Q-Q plots, residual plots, and goodness of fit (GoF) tests are generally ill-suited. For example, we can plot residuals against fitted values with smoothed scatter plot, as follows. We do not find any noticeable trends for none of the models; even the spline outperforms the linear model.


## Residual vs. Fitted Value

We can check the residuals against the fitted value with a smooth scatter plot. And we find that there are no apparent trends for both models, even though the spline model has fewer mean square errors.


## Residuals vs. Terms

A possible solution to check the "goodness of fit (GoF)" is to check whether apparent trends in the plots of residual against terms in the models. We can spot a slight trend residual in the BMI range from 20 to 40, indicating that using linear regression on BMI term may not hurt the model performance too much. However, a strong parabola-like trend can be observed in the residuals of the linear model with respect to ages, which indicates that the linear model cannot capture age. In other words, the model is not good enough to be a base model to run EnWAS; the findings are more likely false positives if using such a base model. On the other hand, the residuals spline model has no clear trends with respect to both BMI and age, which means the base model captures the relations of outcomes (diastolic) and the known confounders.

We can further check the base models with binned plots, which can be helpful when the data set is large. The binned plot is a way that "zoom in" to look at the trends.


```{r binned33,results = "asis",warning=FALSE,echo=FALSE}

df_age_res <- list("Linear"=make_bins(x=base_df$RIDAGEYR,y=lm_base$residual,nbin=1000),
                "Spline"=make_bins(x=base_df$RIDAGEYR,y=ns_base$residuals,nbin=1000)
                )
age_res <- plot_bins2(df_age_res,xlab="Age (year)",ylab="Binned Residuals") + ylim(-8,6.5)


# Residuals vs. Fitted Values
pred_df <- data.frame("Age"=base_df$RIDAGEYR, 
                      "Gender"=base_df$RIAGENDR,
                      "BMI"=base_df$BMXBMI,
                      "DIASTOLIC" = base_df$DIASTOLIC,
                      "Linear"=lm_base$fitted.values,
                      "Spline"=ns_base$fitted.values)
mpred_df <- reshape::melt(pred_df, id=c("DIASTOLIC","Age","Gender",'BMI'))
base_raw_g <- ggplot(mpred_df,
         aes(
           x = Age,
           y = DIASTOLIC
         )) +
  geom_point(data = ~ group_by(.x, Age, Gender,BMI) |> sample_frac(0.2),
             alpha = 0.2, shape=1) +
  geom_smooth(aes(x = Age,y=value,colour=variable),size=1.5,
              method='lm',formula=y ~ splines::ns(x,df=7)
              )+
  xlab("Age (year)")+ylab("Diastolic (mmHg)")+ facet_grid(cols = vars(variable))+
   scale_colour_manual(name="Model", values=c("#E69F00", "#56B4E9"))+
  theme_minimal()
pred_df <- data.frame("Age"=base_df$RIDAGEYR, "Gender"=base_df$RIAGENDR, "BMI"=base_df$BMXBMI,"DIASTOLIC" = base_df$DIASTOLIC,"Linear"=lm_base$residuals,"Spline"=ns_base$residuals)
mpred_df <- reshape::melt(pred_df, id=c("DIASTOLIC","Age","Gender",'BMI'))
res_g <- ggplot(
    mpred_df, aes(x = Age, y = value)) +
    stat_density2d(aes(fill = ..density..^0.25), 
                   geom = "tile", contour = FALSE, n = 200) +  
  geom_smooth(aes(x = Age,y=value,colour=variable),
              size=1.5,method='lm',formula=y ~ splines::ns(x,df=7))+
  facet_grid(cols = vars(variable))+scale_colour_manual(name="Model", values=c("#E69F00", "#56B4E9"))+
    viridis::scale_fill_viridis(guide="none",option = "A",alpha = 0.6) +
  ylab("Residuals")+ xlab("Age(year)")+
  theme_minimal() 
tmp_df <- data.frame(Fitted_Value = c(lm_base$fitted.values,ns_base$fitted.values),
                     Residuals = c(lm_base$residuals,ns_base$residuals),
                     model = c(rep("Linear",nrow(base_df)),rep("Spline",nrow(base_df)))
                     )
fitt_g <- ggplot(tmp_df, aes(x = Fitted_Value, y = Residuals)) +
    stat_density2d(aes(fill = ..density..^0.25),
                   geom = "tile", contour = FALSE, n = 200) +
  geom_smooth(aes(x = Fitted_Value, y = Residuals,colour=model),
              size=1.5,method='lm',formula=y ~ splines::ns(x,df=7))+
  facet_grid(cols = vars(model))+
  scale_colour_manual(name="Model", values=c("#E69F00", "#56B4E9"))+
  xlab("Fitted Values")+
    scale_fill_viridis_c(alpha = 0.6,guide = "none") + theme_minimal()
```

Figure \@ref(fig:qaac) shows the process of Quality Assurance for the models.

```{r qaac, fig.cap = "Quality Assurance and Quality Control",fig.align = 'center',fig.width = 10,fig.height=10,echo=FALSE}
ggpubr::ggarrange(base_raw_g,fitt_g,res_g,age_res,nrow = 4,ncol = 1,labels = c('a)','b)','c)','d)'))
```
- **_a)_** The yellow and blue lines are generated by smooth prediction from linear and spline models. The dots are randomly sampled in 20% of the data points.
- **_b)_** > Smooth scatter plots for residuals with respect to fitted values, and there is no strong pattern in both cases even though the spline has less mean square error than the linear model.
- **_c)_**  Smooth scatter plots for residuals with respect to age variable,  and the linear model has a parabola-like pattern, whereas no obvious pattern for the spline model.
- **_d)_** > Binned plots for residuals with respect to age variable to look at the trends of residuals against age.


# Identify a set of phenotypes and exposures 

Identify a set of phenotypes and exposures that we want to test for association with the response variable of interest.

In this example, we choose phenotype from the dietary data, we identified the phenotypes and saved them into a built-in variable call `exposure_vars` in EnWAS package. More detail of the phenotypes can be found [cook book](https://wwwn.cdc.gov/Nchs/Nhanes/2015-2016/DR1TOT_I.htm). 

```{r}
diet_data = unionQuery('DietaryInterviewTotalNutrientIntakesFirstDay',exposure_vars)
```
Once we load the dietary data, we should also check it with the PHESANT function and preprocess it if necessary.


## Transformation
In NHANES, the phenotypes are often right-skewed with a very long tail on the right side of the distribution, which can be addressed with logarithm transformation followed by a z-transformation. However, it would take a tremendous effort to manually and exhaustively inspect the distributions and figure out appropriate transformation methods for all kinds of phenotypes with different types of distribution when dealing with extensive data sets. Therefore, we recommend using inverse normal transformation (INT) for all continuous variables because INT can apply various distributions. We compared the EnWAS results of logarithm transformation followed by a z-transformation with normal inverse transformation. 

Figure \@ref(fig:DR1TTFAT) shows an example of DR1TTFAT - Total fat (gm) with non transformation, log-z transformation, and inverse normal transformation.


```{r DR1TTFAT, fig.cap = "Transformation for Total fat (gm)",fig.align = 'center',out.width = '85%',fig.height=4,echo=FALSE}
plot_trans <- function(x){
  layout(matrix(c(1,2,3), ncol=3, byrow=TRUE))
  par(mar=c(4.0,4.0,3.5,1))
  x <- na.omit(x)
  hist(x,main = "none")
  log(x+1e-5) |> scale() |> hist(main="log-Z")
  invNorm(x) |> hist(main="INVT")  
}
plot_trans(diet_data$DR1TTFAT)
```



# Carry out a set of regression models.
In this step, we need to carry out a set of regression models, one for each exposure/phenotype in Step 5, and report on the analysis.

```{r}
data = merge(base_df,diet_data,by = "SEQN")
data = na.omit(data)
xwas = enwas(ns_str,exposure_vars,data)
xwas_log <- enwas(ns_str,exposure_vars,data,trans = "log")
xwas_inv <- enwas(ns_str,exposure_vars,data,trans = "inv")
```

Figure @\ref(fig:enwas00_forest) shows the estimates and CI of the exposure variables and only displays the top 30 (if they have more than 30 satisfy the filters) ranked by absolute values of the estimates. The variables with their CI containing zeros are also removed.

```{r enwas00_forest, fig.cap = "Forest Plot for non tranformation EnWAS", echo=F,warning=F, out.width = '90%',dpi = 200}
forest_plot(xwas$enwas_res,30) # filter out CI contains 0
```




Figure @\ref(fig:enwas_inv1log)  shows the top 20 exposures, ranked by the differences in the estimates for the same variables.
- `ns` denotes the variables non-transformed, but the estimates  with beta^hat * SD(X)
- `ns_inv` denotes variables transformed inverse normal transformation
- `ns-log` denotes variables transformed with log followed by z-transformation

```{r enwas_inv1log, fig.cap = "Multiple Forest Plots", echo=F,warning=F, out.width = '90%',dpi = 200}
forest_plot_mult(list(ns=xwas$enwas_res,ns_inv=xwas_inv$enwas_res,ns_log=xwas_log$enwas_res),20)
```

The following scatter plot shows the inverse normal transformation estimates against estimates (beta^hat * SD(X)) of nontransformed variables. The top 20 has added text for the variables, but it is pretty clear to show the information.

```{r enwas_inv22, echo=TRUE,warning=F, echo=F,out.width = '90%',dpi = 200}
ns=xwas$enwas_res
ns_inv=xwas_inv$enwas_res
ns_log=xwas_log$enwas_res
enwas_res = data.frame(x=ns$estimate,
                       x_upper = ns$upper,
                       x_lower = ns$lower,
                       y=ns_inv$estimate,
                       y_upper=ns_inv$upper,
                       y_lower=ns_inv$lower,
                       z=ns_log$estimate,
                       z_upper=ns_log$upper,
                       z_lower=ns_log$lower,
                       diff1 = abs(ns_inv$estimate-ns$estimate),
                       diff2 = abs(ns_log$estimate-ns$estimate),
                       diff3 = abs(ns_log$estimate-ns_inv$estimate),
                       term = ns_inv$term
                       )
```


Figure @\ref(fig:enwas_inv230) estimates of EnWAS  non-transformed (`EnWAS`) with inverse normal transformation (`EnWAS INT`). The the top 20 most difference variables label with variable names.

```{r enwas_inv230, fig.cap = "Scatter Plot for EnWAS against EnWAS with INT", warning=FALSE,echo=F, out.width = '90%',dpi = 200}
library(ggrepel)
top_n_diff <- 20
enwas_res |>
ggplot(aes(x,y,label = term,colour=term)) +
  geom_point(size=1.5)+
  geom_smooth(aes(x,y,colour=NULL),method = "lm", formula = y~x)+
  geom_text_repel(data=dplyr::top_n(enwas_res,top_n_diff,diff1),aes(label=term))+
  theme_minimal()+
  theme(legend.position = "none")+xlab("EnWAS") + ylab("EnWAS INT")
```



We can also compare the non-transformed with log-z transformation, and log-z transformation with inverse normal transformation.



